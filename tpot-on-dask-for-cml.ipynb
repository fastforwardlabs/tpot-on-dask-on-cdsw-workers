{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT on Dask on CDSW Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask[complete]==2021.2.0\n",
      "  Downloading dask-2021.2.0-py3-none-any.whl (900 kB)\n",
      "\u001b[K     |████████████████████████████████| 900 kB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dask-glm==0.2.0\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting dask-ml==1.8.0\n",
      "  Downloading dask_ml-1.8.0-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 77.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 78.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting TPOT==0.11.7\n",
      "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 656 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.24.1\n",
      "  Downloading scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 106.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640 kB)\n",
      "\u001b[K     |████████████████████████████████| 640 kB 102.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toolz>=0.8.2; extra == \"complete\"\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 388 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=0.6.0; extra == \"complete\"\n",
      "  Downloading fsspec-0.8.5-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle>=0.2.2; extra == \"complete\"\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting partd>=0.3.10; extra == \"complete\"\n",
      "  Downloading partd-1.1.0-py3-none-any.whl (19 kB)\n",
      "Collecting bokeh!=2.0.0,>=1.0.0; extra == \"complete\"\n",
      "  Downloading bokeh-2.2.3.tar.gz (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 76.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting distributed>=2.0; extra == \"complete\"\n",
      "  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
      "\u001b[K     |████████████████████████████████| 675 kB 95.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.25.0; extra == \"complete\" in /usr/local/lib/python3.6/site-packages (from dask[complete]==2021.2.0) (0.25.1)\n",
      "Collecting multipledispatch>=0.4.9\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/site-packages (from dask-glm==0.2.0) (1.5.2)\n",
      "Collecting numba\n",
      "  Downloading numba-0.52.0-cp36-cp36m-manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 99.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/site-packages (from dask-ml==1.8.0) (20.4)\n",
      "Collecting stopit>=1.1.1\n",
      "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
      "Collecting deap>=1.2\n",
      "  Downloading deap-1.3.1-cp36-cp36m-manylinux2010_x86_64.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 100.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.36.1\n",
      "  Downloading tqdm-4.57.0-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 184 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting xgboost>=1.1.0\n",
      "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5 MB 67 kB/s /s eta 0:00:01     |███████████████████████▊        | 117.0 MB 113.2 MB/s eta 0:00:01     |█████████████████████████████▌  | 145.4 MB 113.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting update-checker>=0.16\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting joblib>=0.13.2\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 86.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting locket\n",
      "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]==2021.2.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.7 in /usr/local/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]==2021.2.0) (2.11.2)\n",
      "Collecting pillow>=7.1.0\n",
      "  Downloading Pillow-8.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 72.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tornado>=5.1 in /usr/local/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]==2021.2.0) (6.0.4)\n",
      "Collecting typing_extensions>=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.0.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]==2021.2.0) (40.6.2)\n",
      "Collecting click>=6.6\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 134 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting contextvars; python_version < \"3.7\"\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 87.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (272 kB)\n",
      "\u001b[K     |████████████████████████████████| 272 kB 85.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.25.0; extra == \"complete\"->dask[complete]==2021.2.0) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/site-packages (from multipledispatch>=0.4.9->dask-glm==0.2.0) (1.15.0)\n",
      "Collecting llvmlite<0.36,>=0.35.0\n",
      "  Downloading llvmlite-0.35.0-cp36-cp36m-manylinux2010_x86_64.whl (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 76.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging->dask-ml==1.8.0) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.3.0 in /usr/local/lib/python3.6/site-packages (from update-checker>=0.16->TPOT==0.11.7) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from Jinja2>=2.7->bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]==2021.2.0) (1.1.1)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.15-cp36-cp36m-manylinux1_x86_64.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->TPOT==0.11.7) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->TPOT==0.11.7) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->TPOT==0.11.7) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->TPOT==0.11.7) (2.8)\n",
      "Building wheels for collected packages: bokeh, stopit, contextvars\n",
      "  Building wheel for bokeh (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bokeh: filename=bokeh-2.2.3-py3-none-any.whl size=9296309 sha256=eabf399512078796b27a35a987695b8b1ca29900c6fc3b0292b126ca2be66f4f\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/36/60/6b/2a439a4e4b2cb34846f97b81687bc8a6bb3a96c5574fb5dd6c\n",
      "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11956 sha256=00754e76adc3e1bb5bbaf5f71756a842026db63444995478709a6998a90291ea\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/07/2e/ce/e558b7d4f9aafcdc0e5638ef890a3d5166d8a0f2c2dc768379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for contextvars (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7664 sha256=361d4f8cc03d232dd04807f9b1d8a23c09ff94fba80180e1f5879140763ea746\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/41/11/53/911724983aa48deb94792432e14e518447212dd6c5477d49d3\n",
      "Successfully built bokeh stopit contextvars\n",
      "Installing collected packages: pyyaml, toolz, fsspec, cloudpickle, locket, partd, numpy, pillow, typing-extensions, bokeh, sortedcontainers, tblib, heapdict, zict, click, immutables, contextvars, psutil, msgpack, distributed, dask, multipledispatch, joblib, threadpoolctl, scikit-learn, dask-glm, llvmlite, numba, dask-ml, stopit, deap, tqdm, xgboost, update-checker, TPOT\n",
      "Successfully installed TPOT-0.11.7 bokeh-2.2.3 click-7.1.2 cloudpickle-1.6.0 contextvars-2.4 dask-2021.2.0 dask-glm-0.2.0 dask-ml-1.8.0 deap-1.3.1 distributed-2021.2.0 fsspec-0.8.5 heapdict-1.0.1 immutables-0.15 joblib-1.0.1 llvmlite-0.35.0 locket-0.2.1 msgpack-1.0.2 multipledispatch-0.6.0 numba-0.52.0 numpy-1.19.5 partd-1.1.0 pillow-8.1.0 psutil-5.8.0 pyyaml-5.4.1 scikit-learn-0.24.1 sortedcontainers-2.3.0 stopit-1.1.2 tblib-1.7.0 threadpoolctl-2.1.0 toolz-0.11.1 tqdm-4.57.0 typing-extensions-3.7.4.3 update-checker-0.18.0 xgboost-1.3.3 zict-2.0.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade dask[complete]==2021.2.0 dask-glm==0.2.0 dask-ml==1.8.0 numpy==1.19.5 TPOT==0.11.7 scikit-learn==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade dask[complete]==2021.01.1 dask-ml==1.8.0 tpot==0.11.7\n",
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.6/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cdsw\n",
    "from dask.distributed import Client\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make two directories that are needed by Dask. Dask uses these directories to share network information between the scheduler and workers. From the user perspective, create them and forget them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"_scheduler_\", exist_ok=True)\n",
    "os.makedirs(\"_worker_\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dask scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start a Dask scheduler as a CDSW worker process. The scheduler is responsible for coordinating work between the workers. Later we'll start a client in this notebook. The client talks to the scheduler, and the scheduler talks to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_scheduler = cdsw.launch_workers(\n",
    "  n=1,\n",
    "  cpu=1,\n",
    "  memory=2,\n",
    "  kernel=\"python3\",\n",
    "  code=f\"!dask-scheduler --host 0.0.0.0 --dashboard-address 127.0.0.1:8090 --scheduler-file /home/cdsw/_scheduler_/dask.log\"\n",
    ")\n",
    "\n",
    "# Wait for the scheduler to start.\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the IP address of worker with the scheduler on it, so we can connect the dask workers to it. The IP is not returned in the `dask_scheduler` object (it's unknown at the launch of the scheduler), so we scan through the worker list and find the IP of the worker with the scheduler `id`. This returns a list, but there should be only one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tcp://100.100.29.218:8786'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_workers = cdsw.list_workers()\n",
    "scheduler_id = dask_scheduler[0]['id']\n",
    "scheduler_ip = [worker['ip_address'] for worker in scheduler_workers\n",
    "                if worker['id'] == scheduler_id][0]\n",
    "\n",
    "scheduler_url = f\"tcp://{scheduler_ip}:8786\"\n",
    "\n",
    "scheduler_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dask workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start some CDSW workers, each with one dask worker process on it. We pass the scheduler URL we just found so that the scheduler can distribute work to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_workers = cdsw.launch_workers(\n",
    "  n=10,\n",
    "  cpu=1,\n",
    "  memory=2,\n",
    "  kernel=\"python3\",\n",
    "  code=f\"!dask-worker {scheduler_url} --local-directory /home/cdsw/_worker_\"\n",
    ")\n",
    "\n",
    "# Wait for the workers to start.\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Dask client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a local client and connect it to our scheduler. This is how we'll talk to the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(scheduler_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view some stats about the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://100.100.29.218:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://100.100.29.218:8090/status' target='_blank'>http://100.100.29.218:8090/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>160</li>\n",
       "  <li><b>Memory: </b>20.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://100.100.29.218:8786' processes=10 threads=160, memory=20.00 GB>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct URL of Dask dashboard, which is hosted from a worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://5yq79laboko7iqyv.ml-18a296af-d86.demo-aws.ylcu-atmi.cloudera.site/status\n"
     ]
    }
   ],
   "source": [
    "print('//'.join(dask_scheduler[0]['app_url'].split('//'))+ 'status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some data. We're just setting up pipelines here so the data isn't important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define estimator (using Dask!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a TPOT classifier. TPOT is rather sophisticated, and will search over many possible pipelines of sklearn preprocessors and estimators. All we have to do to use the Dask cluster is pass the `use_dask=True` flag, and it'll connect via the client we defined (we do not need to (and cannot) explicitly pass the client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TPOTClassifier(generations=5, population_size=20, use_dask=True, verbosity=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit estimator (using Dask workers!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the `TPOTClassifier`. TPOT tries `population_size` pipeline combinations, then collects the results, and chooses new combinations in a smart way (it's an evolutionary algorithm). It repeats this `generations` times. For each pipeline, it uses 10-fold cross-validation. This is a lot of compute (to do it properly, expect hours or days), so we have restricted to a mere 5 generations, each with population 20. We can stope the process at any point, and TPOT will output the best performing pipeline to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b71ea453b094daa998bec7170ca4eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9732810133553628\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export(\"testimator.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop workers. Stop only those that we started, not all the workers on the cluster, that others may be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdsw.stop_workers(*[worker['id'] for worker in dask_workers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdsw.stop_workers(*[worker['id'] for worker in dask_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
